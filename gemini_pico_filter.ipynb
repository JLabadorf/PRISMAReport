{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache & Conquer: Accelerating Research Analysis with Gemini and Caching\n",
    "This Python notebook uses the Google Gemini API to process a set of PDF research papers and evaluate them against pre-defined inclusion/exclusion criteria. It leverages the Gemini `models/gemini-1.5-flash-001` model with a context cache to efficiently handle multiple documents.\n",
    "\n",
    "Here's a breakdown of the notebook's functionality:\n",
    "\n",
    "1. **Setup and Configuration:**\n",
    "    - Loads environment variables (API key).\n",
    "    - Configures the Gemini API.\n",
    "    - Defines a UUID (`QUERY_UUID`) for organizing files.\n",
    "    - Loads a main prompt from `instructions.txt`.\n",
    "\n",
    "2. **Context Cache Creation:**\n",
    "    - Uploads PDF files from the `./cache/` directory to Gemini's context cache.\n",
    "    - Creates a cached content object with the main prompt and uploaded files, setting a 2-hour TTL. This allows the model to access the content of these files without repeatedly uploading them.\n",
    "\n",
    "3. **Model Initialization:**\n",
    "    - Initializes a Gemini generative model instance using the cached content and specified generation configurations (temperature, top_p, top_k, max output tokens, response schema, and MIME type).  The response schema enforces a structured JSON output including fields like title, author, rationales for different criteria, and boolean pass/fail flags.\n",
    "\n",
    "4. **Processing Individual Papers:**\n",
    "    - Loads PDF files from a directory based on the `QUERY_UUID` (`pdfs/{QUERY_UUID}/`).\n",
    "    - Iterates through each PDF:\n",
    "        - Uploads the PDF to Gemini.\n",
    "        - Constructs a combined prompt using content from `paper_prompt.txt` and `inclusionExclusionCriteria.txt`. This prompt, along with the current PDF, is sent to the Gemini model.\n",
    "        - Parses the JSON response from the model, extracting relevant information.\n",
    "        - Prints the extracted title.\n",
    "        - Stores the JSON response.\n",
    "        - Prints usage metadata for the first request.\n",
    "\n",
    "5. **Dataframe Creation and Export:**\n",
    "    - Creates a Pandas DataFrame from the collected JSON responses.\n",
    "    - Sets \"PaperID\" as the index column.\n",
    "    - Reorders columns to a specific arrangement.\n",
    "    - Saves the DataFrame to a CSV file (`pdfs/{QUERY_UUID}/responses.csv`).\n",
    "\n",
    "\n",
    "**Key Features and Improvements:**\n",
    "\n",
    "- **Context Caching:** Improves efficiency by storing uploaded files in a cache, reducing upload overhead for subsequent requests.\n",
    "- **Structured Output:** Uses a response schema to ensure consistent, structured JSON output from the model.\n",
    "- **Organized File Management:** Uses a `QUERY_UUID` to organize files and results.\n",
    "- **Clearer Variable Names:**  Uses descriptive variable names for improved readability.\n",
    "- **Error Handling:** While not explicitly included, the code could be improved by adding error handling (e.g., `try-except` blocks) to handle potential issues during file processing and API calls.\n",
    "\n",
    "This notebook provides a structured and efficient approach to processing multiple PDF documents using the Google Gemini API and evaluating them based on specific criteria.  The output CSV file provides a summarized overview of the analysis for each paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "QUERY_UUID=\"debd9b3c-4531-462c-b2c2-983b2710fe81\"\n",
    "\n",
    "PROMPT= open(\"instructions.txt\", \"r\").read()\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
    "\n",
    "base = f\"./cache/\"\n",
    "paths = os.listdir(base)\n",
    "files = []\n",
    "\n",
    "for path in paths:\n",
    "    files.append(genai.upload_file(path=base+path))\n",
    "\n",
    "#unfortunately, the model is not able to handle more than 500k tokens at a time. We will limit the number of papers to 10 for now.\n",
    "#This is a limitation of the context cache. \n",
    "num_papers = len(paths)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_schema\": content.Schema(\n",
    "    type = content.Type.OBJECT,\n",
    "    required = [\"title\", \"first author name\", \"populationRationale\", \"interventionExposureRationale\", \"comparatorRationale\", \"outcomeRationale\", \"populationPass\", \"interventionExposurePass\", \"comparatorPass\", \"outcomePass\"],\n",
    "    properties = {\n",
    "      \"title\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"first author name\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"populationRationale\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"interventionExposureRationale\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"comparatorRationale\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"outcomeRationale\": content.Schema(\n",
    "        type = content.Type.STRING,\n",
    "      ),\n",
    "      \"populationPass\": content.Schema(\n",
    "        type = content.Type.BOOLEAN,\n",
    "      ),\n",
    "      \"interventionExposurePass\": content.Schema(\n",
    "        type = content.Type.BOOLEAN,\n",
    "      ),\n",
    "      \"comparatorPass\": content.Schema(\n",
    "        type = content.Type.BOOLEAN,\n",
    "      ),\n",
    "      \"outcomePass\": content.Schema(\n",
    "        type = content.Type.BOOLEAN,\n",
    "      ),\n",
    "    },\n",
    "  ),\n",
    "  \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Create a cache with a 2 hour TTL\n",
    "cache = caching.CachedContent.create(\n",
    "    model='models/gemini-1.5-flash-001',\n",
    "    display_name='cache', # used to identify the cache\n",
    "    system_instruction=(PROMPT),\n",
    "    contents=files,\n",
    "    ttl=datetime.timedelta(hours=2),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache,  generation_config=generation_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = 0\n",
    "\n",
    "base = f\"pdfs/{QUERY_UUID}/\"\n",
    "paths = os.listdir(base)\n",
    "files = []\n",
    "\n",
    "#drop all the non-pdf files\n",
    "for path in paths:\n",
    "    if not path.endswith(\".pdf\"):\n",
    "        paths.remove(path)\n",
    "\n",
    "responses = []\n",
    "#upload each paper and ask the model to generate a response. The promp is in the paper_prompt.txt file\n",
    "for path in paths:\n",
    "    files.append(genai.upload_file(path=base+path))\n",
    "\n",
    "#now we will generate the responses for each paper\n",
    "prompt = open(\"paper_prompt.txt\", \"r\").read()\n",
    "prompt += \"\\n\\n\" + open(\"inclusionExclusionCriteria.txt\", \"r\").read()\n",
    "for paper in files:\n",
    "    response = model.generate_content([paper, prompt])\n",
    "    response_json = json.loads(response.candidates[0].content.parts[0].text)\n",
    "    #print the response title\n",
    "    print(response_json[\"title\"])\n",
    "    responses.append(response_json)\n",
    "    if count == 0:\n",
    "        print(response.usage_metadata)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the responses, we need to create a dataframe that will be used to create the final report\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "#name the Index column PaperID\n",
    "df.index.name = \"PaperID\"\n",
    "\n",
    "#reorder the columns to  [\"title\", \"first author name\", \"populationRationale\", \"interventionExposureRationale\", \"comparatorRationale\", \"outcomeRationale\", \"populationPass\", \"interventionExposurePass\", \"comparatorPass\", \"outcomePass\"]\n",
    "df = df[[\"title\", \"first author name\", \"populationRationale\", \"interventionExposureRationale\", \"comparatorRationale\", \"outcomeRationale\", \"populationPass\", \"interventionExposurePass\", \"comparatorPass\", \"outcomePass\"]]\n",
    "#save the dataframe to a csv file\n",
    "df.to_csv(f\"pdfs/{QUERY_UUID}/responses.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long-context-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
